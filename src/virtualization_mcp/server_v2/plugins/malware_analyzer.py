"""AI-Powered Malware Analysis plugin for virtualization-mcp."""
import asyncio
import hashlib
import logging
import os
import shutil
import tempfile
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Deque, Tuple, BinaryIO

import aiofiles
from fastapi import APIRouter, HTTPException, status, UploadFile, File, BackgroundTasks
from pydantic import BaseModel, Field, HttpUrl

from virtualization_mcp.server_v2.plugins.base import BasePlugin
from virtualization_mcp.server_v2.plugins import register_plugin

logger = logging.getLogger(__name__)

class AnalysisStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class ThreatLevel(str, Enum):
    BENIGN = "benign"
    SUSPICIOUS = "suspicious"
    MALICIOUS = "malicious"
    CRITICAL = "critical"

class Detection(BaseModel):
    name: str
    threat_level: ThreatLevel
    confidence: float = Field(..., ge=0, le=1.0)
    description: str
    indicators: List[Dict[str, Any]] = []

class AnalysisResult(BaseModel):
    analysis_id: str
    filename: str
    file_hash: str
    file_size: int
    mime_type: str
    status: AnalysisStatus = AnalysisStatus.PENDING
    threat_level: ThreatLevel = ThreatLevel.BENIGN
    score: float = 0.0
    detections: List[Detection] = []
    metadata: Dict[str, Any] = {}
    created_at: datetime = Field(default_factory=datetime.utcnow)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

@register_plugin("malware_analyzer")
class MalwareAnalyzerPlugin(BasePlugin):
    """AI-Powered Malware Analysis plugin for virtualization-mcp."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # Configuration
        self.quarantine_dir = Path(config.get("quarantine_dir", "./quarantine"))
        self.analysis_dir = Path(config.get("analysis_dir", "./analysis"))
        self.max_file_size = config.get("max_file_size", 100 * 1024 * 1024)  # 100MB
        self.retention_days = config.get("retention_days", 30)
        
        # Ensure directories exist
        self.quarantine_dir.mkdir(parents=True, exist_ok=True)
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # In-memory storage for analysis results
        self.analyses: Dict[str, AnalysisResult] = {}
        
        # Background tasks
        self.cleanup_task = None
        
        # Set up routes
        self.setup_routes()
    
    def setup_routes(self) -> None:
        """Set up API routes for malware analysis."""
        @self.router.post("/analyze", response_model=AnalysisResult)
        async def analyze_file(
            file: UploadFile = File(...),
            background_tasks: BackgroundTasks = None
        ) -> AnalysisResult:
            """Analyze a file for malware."""
            try:
                # Validate file size
                file.file.seek(0, 2)  # Seek to end of file
                file_size = file.file.tell()
                file.file.seek(0)  # Reset file pointer
                
                if file_size > self.max_file_size:
                    raise HTTPException(
                        status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                        detail=f"File size exceeds maximum allowed size of {self.max_file_size} bytes"
                    )
                
                # Calculate file hash
                file_hash = await self._calculate_file_hash(file.file)
                file.file.seek(0)  # Reset file pointer
                
                # Check if we already analyzed this file
                existing_analysis = await self._find_existing_analysis(file_hash)
                if existing_analysis:
                    return existing_analysis
                
                # Create a new analysis
                analysis_id = f"mal_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{file_hash[:8]}"
                
                # Save file to quarantine
                quarantine_path = await self._quarantine_file(file, analysis_id)
                
                # Create analysis result
                analysis = AnalysisResult(
                    analysis_id=analysis_id,
                    filename=file.filename,
                    file_hash=file_hash,
                    file_size=file_size,
                    mime_type=file.content_type or "application/octet-stream",
                    status=AnalysisStatus.PENDING,
                    metadata={
                        "original_filename": file.filename,
                        "content_type": file.content_type,
                        "quarantine_path": str(quarantine_path)
                    }
                )
                
                # Store analysis
                self.analyses[analysis_id] = analysis
                
                # Start analysis in background
                background_tasks.add_task(self._analyze_file, analysis_id, quarantine_path)
                
                return analysis
                
            except Exception as e:
                logger.error(f"Error analyzing file: {str(e)}", exc_info=True)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Error analyzing file: {str(e)}"
                )
        
        @self.router.get("/analyses/{analysis_id}", response_model=AnalysisResult)
        async def get_analysis(analysis_id: str) -> AnalysisResult:
            """Get analysis results by ID."""
            if analysis_id not in self.analyses:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Analysis '{analysis_id}' not found"
                )
            
            return self.analyses[analysis_id]
        
        @self.router.get("/analyses", response_model=List[AnalysisResult])
        async def list_analyses(
            limit: int = 100,
            offset: int = 0,
            status: Optional[AnalysisStatus] = None,
            threat_level: Optional[ThreatLevel] = None
        ) -> List[AnalysisResult]:
            """List all analyses with optional filtering."""
            results = list(self.analyses.values())
            
            # Apply filters
            if status is not None:
                results = [r for r in results if r.status == status]
            
            if threat_level is not None:
                results = [r for r in results if r.threat_level == threat_level]
            
            # Sort by creation date (newest first)
            results.sort(key=lambda x: x.created_at, reverse=True)
            
            # Apply pagination
            return results[offset:offset + limit]
        
        @self.router.delete("/analyses/{analysis_id}")
        async def delete_analysis(analysis_id: str) -> Dict[str, str]:
            """Delete an analysis and associated files."""
            if analysis_id not in self.analyses:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Analysis '{analysis_id}' not found"
                )
            
            # Remove from memory
            analysis = self.analyses.pop(analysis_id)
            
            # Delete associated files
            try:
                quarantine_path = Path(analysis.metadata.get("quarantine_path", ""))
                if quarantine_path.exists():
                    quarantine_path.unlink()
                
                analysis_dir = self.analysis_dir / analysis_id
                if analysis_dir.exists():
                    shutil.rmtree(analysis_dir)
            except Exception as e:
                logger.error(f"Error cleaning up analysis files: {str(e)}")
            
            return {"status": "deleted", "analysis_id": analysis_id}
        
        @self.router.get("/quarantine/list")
        async def list_quarantine() -> List[Dict[str, Any]]:
            """List all files in quarantine."""
            quarantined = []
            
            for file_path in self.quarantine_dir.glob("*"):
                if not file_path.is_file():
                    continue
                
                stat = file_path.stat()
                quarantined.append({
                    "filename": file_path.name,
                    "size": stat.st_size,
                    "created_at": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    "modified_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "analysis_id": file_path.stem
                })
            
            return sorted(quarantined, key=lambda x: x["created_at"], reverse=True)
    
    async def _calculate_file_hash(self, file: BinaryIO) -> str:
        """Calculate SHA-256 hash of a file."""
        sha256 = hashlib.sha256()
        
        # Read file in chunks to handle large files
        for chunk in iter(lambda: file.read(8192), b""):
            sha256.update(chunk)
        
        return sha256.hexdigest()
    
    async def _quarantine_file(self, file: UploadFile, analysis_id: str) -> Path:
        """Save a file to quarantine."""
        # Create a safe filename
        safe_filename = f"{analysis_id}_{file.filename}"
        safe_filename = "".join(c for c in safe_filename if c.isalnum() or c in "._-")
        
        # Save file to quarantine
        quarantine_path = self.quarantine_dir / safe_filename
        
        async with aiofiles.open(quarantine_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
        
        return quarantine_path
    
    async def _find_existing_analysis(self, file_hash: str) -> Optional[AnalysisResult]:
        """Find an existing analysis by file hash."""
        for analysis in self.analyses.values():
            if analysis.file_hash == file_hash:
                return analysis
        return None
    
    async def _analyze_file(self, analysis_id: str, file_path: Path) -> None:
        """Analyze a file for malware (background task)."""
        if analysis_id not in self.analyses:
            logger.error(f"Analysis ID not found: {analysis_id}")
            return
        
        analysis = self.analyses[analysis_id]
        analysis.status = AnalysisStatus.RUNNING
        analysis.started_at = datetime.utcnow()
        
        try:
            # Create analysis directory
            analysis_dir = self.analysis_dir / analysis_id
            analysis_dir.mkdir(exist_ok=True)
            
            # Analyze the file
            detections = await self._perform_analysis(file_path, analysis_dir)
            
            # Update analysis result
            analysis.detections = detections
            analysis.status = AnalysisStatus.COMPLETED
            
            # Calculate overall threat level and score
            threat_levels = {
                ThreatLevel.BENIGN: 0,
                ThreatLevel.SUSPICIOUS: 0,
                ThreatLevel.MALICIOUS: 0,
                ThreatLevel.CRITICAL: 0
            }
            
            for detection in detections:
                threat_levels[detection.threat_level] += 1
            
            # Determine overall threat level
            if threat_levels[ThreatLevel.CRITICAL] > 0:
                analysis.threat_level = ThreatLevel.CRITICAL
            elif threat_levels[ThreatLevel.MALICIOUS] > 0:
                analysis.threat_level = ThreatLevel.MALICIOUS
            elif threat_levels[ThreatLevel.SUSPICIOUS] > 0:
                analysis.threat_level = ThreatLevel.SUSPICIOUS
            else:
                analysis.threat_level = ThreatLevel.BENIGN
            
            # Calculate score (0-100)
            analysis.score = min(100, len(detections) * 10)
            
            # Log completion
            logger.info(f"Analysis completed for {analysis_id}: {analysis.threat_level}")
            
        except Exception as e:
            logger.error(f"Error analyzing file: {str(e)}", exc_info=True)
            analysis.status = AnalysisStatus.FAILED
            analysis.metadata["error"] = str(e)
        
        finally:
            analysis.completed_at = datetime.utcnow()
    
    async def _perform_analysis(
        self,
        file_path: Path,
        analysis_dir: Path
    ) -> List[Detection]:
        """Perform actual malware analysis."""
        detections = []
        
        # 1. Check file type
        file_type = await self._get_file_type(file_path)
        
        # 2. Extract metadata
        metadata = await self._extract_metadata(file_path, file_type)
        
        # 3. Run static analysis
        static_analysis = await self._run_static_analysis(file_path, analysis_dir)
        
        # 4. Run dynamic analysis in sandbox (simulated)
        dynamic_analysis = await self._run_dynamic_analysis(file_path, analysis_dir)
        
        # 5. Generate detections based on analysis
        if file_type == "PE32" and not any(d.name == "PE32_Header" for d in detections):
            detections.append(Detection(
                name="PE32_Header",
                threat_level=ThreatLevel.SUSPICIOUS,
                confidence=0.8,
                description="PE32 executable detected. This could be a Windows executable.",
                indicators=[{"type": "file_type", "value": file_type}]
            ))
        
        # Add more detection logic here based on analysis results
        
        return detections
    
    async def _get_file_type(self, file_path: Path) -> str:
        """Determine file type using magic numbers."""
        # In a real implementation, this would use a library like python-magic
        # For now, we'll just check the extension
        ext = file_path.suffix.lower()
        
        if ext in ['.exe', '.dll', '.sys']:
            return "PE32"
        elif ext in ['.py', '.pyc']:
            return "Python"
        elif ext in ['.js', '.jse']:
            return "JavaScript"
        elif ext in ['.ps1', '.psm1', '.psd1']:
            return "PowerShell"
        else:
            return "Unknown"
    
    async def _extract_metadata(self, file_path: Path, file_type: str) -> Dict[str, Any]:
        """Extract metadata from the file."""
        metadata = {
            "file_type": file_type,
            "size": file_path.stat().st_size,
            "created": datetime.fromtimestamp(file_path.stat().st_ctime).isoformat(),
            "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        }
        
        # Add more metadata extraction based on file type
        
        return metadata
    
    async def _run_static_analysis(
        self,
        file_path: Path,
        analysis_dir: Path
    ) -> Dict[str, Any]:
        """Run static analysis on the file."""
        # In a real implementation, this would:
        # 1. Extract strings
        # 2. Disassemble the binary
        # 3. Analyze imports/exports
        # 4. Check for packers/obfuscation
        
        # For now, we'll just return a placeholder
        return {
            "analysis_type": "static",
            "status": "completed",
            "findings": []
        }
    
    async def _run_dynamic_analysis(
        self,
        file_path: Path,
        analysis_dir: Path
    ) -> Dict[str, Any]:
        """Run dynamic analysis in a sandbox."""
        # In a real implementation, this would:
        # 1. Launch a sandboxed environment
        # 2. Execute the file
        # 3. Monitor system calls, network activity, etc.
        # 4. Generate a behavior report
        
        # For now, we'll just return a placeholder
        return {
            "analysis_type": "dynamic",
            "status": "completed",
            "findings": [],
            "sandbox": "simulated"
        }
    
    async def _cleanup_old_analyses(self) -> None:
        """Clean up old analysis results and files."""
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        
        # Clean up old analyses in memory
        to_remove = [
            analysis_id for analysis_id, analysis in self.analyses.items()
            if analysis.created_at < cutoff
        ]
        
        for analysis_id in to_remove:
            await self.delete_analysis(analysis_id)
        
        # Clean up old files in quarantine and analysis directories
        for dir_path in [self.quarantine_dir, self.analysis_dir]:
            for item in dir_path.iterdir():
                if item.is_file():
                    stat = item.stat()
                    if datetime.fromtimestamp(stat.st_ctime) < cutoff:
                        try:
                            item.unlink()
                        except Exception as e:
                            logger.error(f"Error deleting {item}: {str(e)}")
                elif item.is_dir():
                    stat = item.stat()
                    if datetime.fromtimestamp(stat.st_ctime) < cutoff:
                        try:
                            shutil.rmtree(item)
                        except Exception as e:
                            logger.error(f"Error deleting directory {item}: {str(e)}")
    
    async def startup(self) -> None:
        """Startup tasks."""
        await super().startup()
        
        # Start cleanup task
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        logger.info("Malware Analyzer plugin started")
    
    async def shutdown(self) -> None:
        """Shutdown tasks."""
        # Cancel cleanup task
        if self.cleanup_task and not self.cleanup_task.done():
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
        
        await super().shutdown()
        logger.info("Malware Analyzer plugin stopped")
    
    async def _cleanup_loop(self) -> None:
        """Background task to clean up old analyses."""
        while True:
            try:
                await self._cleanup_old_analyses()
                # Run cleanup once per day
                await asyncio.sleep(24 * 60 * 60)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in cleanup loop: {str(e)}", exc_info=True)
                # Wait longer on error
                await asyncio.sleep(60 * 60)  # 1 hour
